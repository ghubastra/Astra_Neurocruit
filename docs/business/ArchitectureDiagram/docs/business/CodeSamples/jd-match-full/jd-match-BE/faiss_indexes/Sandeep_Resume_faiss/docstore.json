[[["f10cfa4a-fb08-45f4-bd79-64a968cfb9b8",{"pageContent":"SANDEEP SHARMA \n214-458-9076 |  sandygunnu1309@gmail.com | Dallas, TX \n \nSKILLS \n• \nCloud - AWS | GCP \n• \nBig Data - HDFS | Spark | | Hive | Scala | Impala \n• \nDB - Oracle | SQLServer | Sybase | MySQL | PG \n• \nMessaging - Kafka | JMS | IBM MQ | Oracle AQ \n• \nLanguages - Java | Python | Perl | Shell | XML \n• \nCI/CD - Ant | Maven | Hudson | Jenkins | Cruise \nControl | Gitlab | JIRA \n• \nPlatform - Unix | Linux | Windows | Solaris \n• \nLogs/Scheduler - CTRL-M | Splunk | Run Deck \n• \nETL - Talend | OWB | SSIS | DTS \n• \nApp/Web Server – Glassfish | WebLogic | \nTomcat  \nCLIENTS WORKED FOR \n• \nDeutsche Bank \n• \nCITI \n• \nJPM Chase \n• \nSAC Capital \n• \nS3 Partners \n• \nQuadriserve \nFINANCIAL SYSTEM WORKED ON \n• \nBasket trading system \n• \nTrade Blotter system \n• \nTrade Reconciliation system \n• \nEclipse portfolio management system \n• \nAQS Middle Office \n \nPROFESSIONAL SUMMARY \n• \nAWS Cloud / Data professional having 18 + plus years of experience specializing \nin Cloud, Data & Analytics.  \n•","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":1,"lines":{"from":4,"to":59}}}}],["1492048b-c12a-4155-a69f-152b51a7cdb0",{"pageContent":"Eclipse portfolio management system \n• \nAQS Middle Office \n \nPROFESSIONAL SUMMARY \n• \nAWS Cloud / Data professional having 18 + plus years of experience specializing \nin Cloud, Data & Analytics.  \n• \nStrong exposure in verticals like BFS, Investment Banking, Telecom & Travel & \nAviation. \n• \nPerformed AWS Cloud/Data engineer role for complete end to end On-Prem \nmigration to AWS Cloud landscape \n• \nImplemented AWS Services - EC2 | RDS | DMS | SCT | Cloud Watch | Cloud \nFormation | Cloud Events | Lambda | Step function | S3 | Redshift etc. \n• \nImplemented AWS DMS – Data Migration service to migrate 12TB of real time \ndata from multiple cloud sources to multiple destinations - AWS RDS systems \n• \nSpin up new infra for AWS Sandbox based env. having AWS Multi Corals / Multi \nNodes with auto scaling setup. \n• \nPerformed various AWS deployment-based activities i.e., 12 factor deployment, \nscript-based execution & troubleshooting deployment issues \n•","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":1,"lines":{"from":51,"to":77}}}}],["34c7b52e-9361-4152-bb5c-6f6eaefc1d3f",{"pageContent":"Nodes with auto scaling setup. \n• \nPerformed various AWS deployment-based activities i.e., 12 factor deployment, \nscript-based execution & troubleshooting deployment issues \n• \nAWS Logs Monitoring – Cloud Watch, Sumo monitoring, Log Analysis, new \ndashboard creation etc. \n• \nWorked closely with AWS CloudOps team to help them build & support Red env. \nDeployment & trouble shoot environmental / deployment issues. \n• \nLiaising across multiple AWS teams i.e., Cloud Ops / DBA / AWS Support to \nhelp resolve various technical / performance related issues \n• \nExperience using DevOps tools in a cloud environment, such as Ansible, \nArtifactory, Docker, GitHub, Jenkins, Kubernetes, Maven etc. \n• \nAWS Comparative Tools & Technologies analysis using different key parameters \nCost, Scalability, HA, Fault Tolerance etc. \n• \nExtensive onsite work experience at client site (US / UK / Australia / Singapore / \nMiddle East). \n•","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":1,"lines":{"from":73,"to":95}}}}],["142ffd59-1863-4af1-ac7d-930b4546e8c7",{"pageContent":"Cost, Scalability, HA, Fault Tolerance etc. \n• \nExtensive onsite work experience at client site (US / UK / Australia / Singapore / \nMiddle East). \n• \nPerformance optimization - Identification application performance bottlenecks and \npropose solutions to eliminate latency to build a fast / scalable system.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":1,"lines":{"from":91,"to":97}}}}],["a456afe3-1833-4a91-834e-95e08f1d9885",{"pageContent":"EDUCATION \nTechnical Certification  \nAWS - Solution Arch. \nAWS - Solution Arch. Professional \nAWS - Big Data Specialty \nAWS - Cloud Practitioner \nSUN – Certified Java Programmer \nMDCA Software \nNICE, India \n \n \nPROFESSIONAL EXPERIENCE \nAWS Cloud | Data Prof. - Associate Director, Aug 2016 – Aug 2023  \nUS Consulting Giant, US  \nTechnical Manager, Oct 2012 - Aug 2016  \nIGT, India  \nSr. Consultant, Nov 2001 - May 2011  \nIVP, India  \n \n \nPROJECTS UNDERTAKEN: - \nTitle AWS Cloud Database / Data Professional \nClient JPMC \nPlatform AWS, RDS, DMS, SCT, DB Migration – Oracle / SQL Server, S3 etc. \nDescription The objective of the JPMC Cloud Migration Project is to move JPMC out of the data center \nin 3 years which will be achieved by evaluating each app and putting it into 1 of 3 categories. \nThe whole scope of the application is to migrate some 390 odd applications out of DC and \nmove to AWS Cloud in a phased manner. \n1. Lift and shift \n2. Re-platform \n3. Re-architect \nAssignments","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":2,"lines":{"from":3,"to":34}}}}],["c0d8adb6-d748-4d71-bf7d-a6b98fa06e3e",{"pageContent":"The whole scope of the application is to migrate some 390 odd applications out of DC and \nmove to AWS Cloud in a phased manner. \n1. Lift and shift \n2. Re-platform \n3. Re-architect \nAssignments \n• Part of assessment / discovery team to understand the on-prem stack and captured \nthe key areas of migration from DB perspective. \n• Worked closely with JPMC business / tech. team to understand their cloud \nlandscape in detail and suggested various DB migration strategies. \n• Primarily part of DB migration team where led a team of DB developers / DBAs to \nlay out the DB migration strategies. \n• Attended various EAB calls with AWS team of architects and worked thru. working \nsession on real time basis. \n• Setup DMS & SCT services to help migrate DB stuff quickly to AWS RDS env. \n• Comparative analysis for DMS Vs Backup Restore DB migration strategies and \nweigh in both options to optimize DB migration. \n• Managed onsite/offshore based team and responsible for end-to-end delivery for","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":2,"lines":{"from":29,"to":46}}}}],["593869ee-0322-4fa1-b8c0-bfc3e76a1a59",{"pageContent":"weigh in both options to optimize DB migration. \n• Managed onsite/offshore based team and responsible for end-to-end delivery for \nmigration \n• Liaising with the JPMC architects & other stake holders to firm up the architecture \nand provide technical guidance to offshore team \n• Data export, loading & validation across Dev | MOD | Prod env. \n• Liaising with DevOps / DBA / JPMC based teams to help resolve Green / Red \nenvironmental issues \n• Planning & executing ramp up plan for existing functionality plus addition of new \nfunctionality. \n \nTitle GCP Cloud Data Professional \nClient Iron Mountain \nPlatform GCP, GCS, BQ, Data Proc, Terraform, Gitlab, Data Lake, Cloud Composer, Astronomer \nDescription IRM Global Data Warehouse project requirement was to create a data lake across Ingestion, \nRefined, Reporting, Exchange layer and get the data from on-prem systems using flat files & \ndirect connectivity from TrSQL Sql Server based databases. Iron mountain (IRM) is a client","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":2,"lines":{"from":45,"to":61}}}}],["28f86679-a787-4fad-b7f4-d9c43374889d",{"pageContent":"having business in the record maintenance for end customers through ‘data’ lifecycle by \nmeans of processing the storage, repository and file processing services for companies since \n1951. \n \nAssignments \n• Worked closely with IRM business / tech. team to understand their cloud landscape \nand help converting requirements into data lake related documents. \n• Managed onsite/offshore based team and responsible for end-to-end delivery for \ndata lake creation. \n• liaising with the IRM architects & other stake holders to firm up the architecture \nand provide technical guidance to offshore team \n• Setup of data pipelines across data lake using Ingestion / Refined / Reporting / \nExchange layer \n• Data loading & validation across Ingestion / Refined layer using BQ tables. \n• Validated the Cloud Composer & Astronomer env to setup/monitor DAGs \n• Liaising with DevOps / DBA / GCP based teams to help resolve Green / Red \nenvironmental issues","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":3,"lines":{"from":3,"to":19}}}}],["c4491f3a-54fe-44bd-918f-2ed25d5d6a45",{"pageContent":"• Validated the Cloud Composer & Astronomer env to setup/monitor DAGs \n• Liaising with DevOps / DBA / GCP based teams to help resolve Green / Red \nenvironmental issues \n• Planning & executing ramp up plan for existing functionality plus addition of new \nfunctionality. \n \nTitle AWS Cloud/Data Engineer \nClient Fidelity \nPlatform AWS, Concourse, Oracle, Docker \nDescription Fidelity FFIO project requirement was to migrate 44 Oracle databases from on-premises \nenvironment to AWS Cloud env. Migration involved 44 FFIO Non-RAC Oracle databases to \nAWS environment using a Factory Model. Besides, other requirements were to upgrade the \ndatabases from version 12.2.0.1(on-premises) to version 19c (AWS RDS). Concourse \ndevops tool was a Fidelity preferred tool to produce a reusable a factory model based RDS \nProvisioning pipeline as well as data migration pipeline. \nAssignments \n• Worked closely with Fidelity tech. team to understand AWS landscape and help","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":3,"lines":{"from":17,"to":33}}}}],["f748318a-e4f2-457d-8205-8055a83ac354",{"pageContent":"Provisioning pipeline as well as data migration pipeline. \nAssignments \n• Worked closely with Fidelity tech. team to understand AWS landscape and help \nconverting requirements into migration documents. \n• Worked thru. the RDS Capacity planning documents to assess / recommend the best \nRDS params basis on existing On-prem setup \n• Developed a Concourse based DevOps pipeline to migrate the production \nenvironment copy to the UAT RDS environment for all 44 databases in a factory \nmodel fashion. \n• Each concourse pipeline can be used multiple times to spin up new RDS instance or \ndata migration with minimal or no human interaction at all. \n• Part of Generic JSON/YAML creation for RDS Provisioning & Data Migration \nconcourse pipeline. \n• Validated the AWS RDS env. post provision thru. Concourse pipeline \n• Perform schema reconciliations and data validations post each run of data migration \n• Migrated up to 3TB of data thru. Concourse based pipeline using Oracle dump \nutility.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":3,"lines":{"from":31,"to":47}}}}],["ba83840f-906a-4a21-90fb-7e8d50dbad8d",{"pageContent":"• Perform schema reconciliations and data validations post each run of data migration \n• Migrated up to 3TB of data thru. Concourse based pipeline using Oracle dump \nutility. \n• Monitoring across various AWS Green / Red based environment & troubleshoot \nvarious environment / application-level issues. \n• Liaising with DevOps / DBA / AWS based teams to help resolve Green / Red \nenvironmental issues \n• Planning & executing ramp up plan for existing functionality plus addition of new \nfunctionality. \n \nTitle AWS Cloud/Data Engineer \nClient Health First \nPlatform AWS, Pega, Postgres, Oracle \nDescription HF Pega to AWS Cloud is a Healthfirst AWS based migration platform, where 12 Pega \nbased applications being moved from Pega Cloud to AWS Cloud platform. The data and","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":3,"lines":{"from":45,"to":59}}}}],["b49d8c1c-a11e-46d5-9ef4-b848e6406572",{"pageContent":"information within Pega applications contain PHI information. Objective was to design \nAWS Control Plane to meet the control requirements for handling PHI information. Claims \n& EnB Applications were first ones to migrate 12 TB of data from Pega Cloud to AWS. \nAssignments \n• Worked closely with HF team to understand AWS landscape & Pega Cloud to help \nconverting requirements into migration documents. \n• Implemented various AWS based services like DMS, RDS, Cloud Watch, Cloud \nEvent, S3 etc. \n• Setup AWS DMS – data migration service to move around 12TB of data from \nmultiple sources to multiple target systems. \n• Thorough understanding of HF AWS environment and setup new Green / Red env.  \n \n• Active participation with CloudOps team to spin up new Pega based env. on AWS \ncloud. \n• Deep monitoring across various AWS Green / Red based environment & \ntroubleshoot various environment / application level issues. \n• Liaising with DevOps / DBA / AWS based teams to help resolve Green / Red","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":4,"lines":{"from":3,"to":19}}}}],["5ec117a7-8547-46f7-81b5-cfd9be542939",{"pageContent":"troubleshoot various environment / application level issues. \n• Liaising with DevOps / DBA / AWS based teams to help resolve Green / Red \nenvironmental issues \n• Helped a team of BA, Developers & Tester to ensure smoothly delivery. \n• Planning & executing ramp up plan for existing functionality plus addition of new \nfunctionality. \n \n \nTitle AWS Cloud/Data Engineer \nClient Medidata Platform \nPlatform AWS, Oracle, Pentaho, Java, REST \nDescription Rave2RODS is a Medidata AWS based platform, where Pharma companies gets onboarded \nto help them expedite their Studies / Trials across various subjects. Entire platform was \nspread out from AWS to Pentaho, wrapping up Java / REST based api services within the \narchitecture. Total 300 URLS (Clinical clients) are onboarded in production environment \nand system has a scaling capability up to 2000 URLS which means system supporting up to \n2000 Oracle schema in parallel fashion","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":4,"lines":{"from":18,"to":34}}}}],["a6d6704f-2da0-4310-a7b8-2f40525590b8",{"pageContent":"and system has a scaling capability up to 2000 URLS which means system supporting up to \n2000 Oracle schema in parallel fashion \nAssignments • Worked closely with Medidata team to understand requirement and converting \nthem into Design documents. \n• Thorough understanding of Medidata AWS environment and setup new Sandbox \nbased env. having multi nodes corals with auto scaling capabilities \n• Developed various AWS based services like Cloud Event, Lambda, Step Function, \nS3 \n• Active participation in DevOps functions 12 Factor Deployment, Sumo Logs \nMonitoring, New Sumo Dashboards, AWS Red Env. Support \n• Deep monitoring across various AWS Green / Red based environment & \ntroubleshoot various environment / application level issues. \n• Liaising with DevOps / DBA / AWS based teams to help resolve Green / Red \nenvironmental issues \n• Anchored the production release process end to end right from BA requirement \nuntil production deployment","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":4,"lines":{"from":33,"to":48}}}}],["a30db374-af18-44cc-9eda-df8afb39ee4e",{"pageContent":"environmental issues \n• Anchored the production release process end to end right from BA requirement \nuntil production deployment \n• AWS Cost optimization & Scanning AWR Report meticulously to identify \noptimization pockets. \n• Helped a team of BA, developers & tester to ensure smoothly delivery. \n• Planning & executing ramp up plan for existing functionality plus addition of new \nfunctionality. \n \nTitle Sr. Tech. Analyst \nClient Ally Bank – Metadata Analysis \nPlatform Cloudera Navigator, Hue, Unix, Hadoop","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":4,"lines":{"from":46,"to":57}}}}],["2b51906f-4819-4062-8ad0-f595fbf91c02",{"pageContent":"Description As part of the new digital transformation effort, there was a demand for creating a data \nmarketplace for the Ally business. There are total eighteen (18) databases within the \nAdvanced Analytic Program (“AAP”) that requires ingestion, profiling, cataloging.  The \ncatalogue will cover the critical metadata information \nAssignments \n• Worked with Ally team closely to understand requirement and converting them into \nDesign documents. \n• Understand Ally technical environment and setup DEV / TEST VM on local env. \n• Anchored the ingestion process to inject data from different sources to HDFS file \nsystem into AWS S3. \n• Identification of business / technical metadata and updating same in Cloudera / \nPodium env. \n• Data profiling & catalogue management across the different LOBs & Data owners. \n• Worked out estimation / project plan with client after converting business \nrequirement into technical requirement. \n• Led a team of developers & tester to ensure smoothly delivery.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":5,"lines":{"from":3,"to":18}}}}],["83c53a86-386a-4955-a9b6-a5dd715211ba",{"pageContent":"• Worked out estimation / project plan with client after converting business \nrequirement into technical requirement. \n• Led a team of developers & tester to ensure smoothly delivery. \n• Planning & executing ramp up plan for existing functionality plus addition of new \nfunctionality. \n• Work stack management & team management to help facilitate team. \n• Regular calls with client for daily status update. \n• Ramped up team & involved in grooming team members with different functional / \ntechnical knowledge to bring them up to speed. \n \n \nTitle Architect \nClient CITI \nPlatform AWS, S3, Big Data Stack, Kafka, HDFS, HBase, Talend, Cloudera, Hue, Unix, Java, Oracle \nDescription Event cloud is an AWS & Kafka based system, which receives the events from multiple \nupstream applications i.e. Citi online banking application, MBOL through Glass box and \nprocess the events and sends the processed events to downstream application like customer","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":5,"lines":{"from":16,"to":32}}}}],["5282733b-8092-493f-8ab8-a3f09ab46823",{"pageContent":"upstream applications i.e. Citi online banking application, MBOL through Glass box and \nprocess the events and sends the processed events to downstream application like customer \ngraph for analysis. This is a real-time messaging system based on high frequency / high \nthroughput confluent Kafka based system. \nAssignments \n• Overall responsible for smooth delivery of Development, Deployment till \nproduction support phase. \n• Worked with multiple CITI stake holder to closely understand AWS cloud-based \nrequirements and converting them into migration documents. \n• Anchored the ingestion process to inject data from different sources to AWS S3 file \nsystem. \n• Designed and developed the Kafka messaging System using apache Kafka \nstreaming api \n• Data migration from On Prem. to AWS S3. \n• Developed the Kafka producer and consumer module. \n• Involved in Kafka cluster setup broker and zookeepers. \n• Developed Talend based pipeline from S3/Flat Files to Oracle destination.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":5,"lines":{"from":31,"to":47}}}}],["d5f0cf35-932d-4e35-8538-305c3f2d2d11",{"pageContent":"• Developed the Kafka producer and consumer module. \n• Involved in Kafka cluster setup broker and zookeepers. \n• Developed Talend based pipeline from S3/Flat Files to Oracle destination. \n• Worked out estimation / project plan with client after converting business \nrequirement into technical requirement. \n• Led a team of developers & tester to ensure smoothly delivery. \n• Planning & executing ramp up plan for existing functionality plus addition of new \nfunctionality. \n• Work stack management & team management to help facilitate team. \n• Regular calls with client for daily status update. \n \nTitle BT B&PS MC – Architect Big Data \nClient BT \nPlatform BT HaaS env., Spark, Hadoop, Hive, Impala, Sqoop, Scala, Oozie, Hue, Linux","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":5,"lines":{"from":45,"to":58}}}}],["d10479dc-931d-4c2e-863a-d97e3be403f1",{"pageContent":"Description BT EE Mobile - Migration of existing BT customer applications as part of Mobile EE DAN, \nCRS, Sim Rewrite etc. with a dynamic rule selection criterion stored in database to the BT \nHaaS env. Idea is to seamlessly migrate the applications with latest technologies without any \ntangible impact to customer applications during migration process. \nAssignments \n• Overall response for smooth delivery of BT HaaS apps from migration perspective \nfrom Design, Development till deployment / support and finally handling over to \nASG team. \n• Planning & executing ramp up plan for existing apps plus addition of new \nfunctionality. \n• Designing and configuring Big Data solutions to handle the large big data problems \n(BT -EE Mobile DAN) \n• Provided oversight for various BT big data application and revamping the \narchitecture to newer technologies for better performance. \n• Providing latest technical Big Data solution for project requirements","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":6,"lines":{"from":3,"to":17}}}}],["3a008d76-21a6-4a6b-a47d-bf3b287374f1",{"pageContent":"architecture to newer technologies for better performance. \n• Providing latest technical Big Data solution for project requirements  \n• Performing Big Data technical requirement feasibility study and implementing \nsolutions. \n• Active participation in regular technical client meetings and suggested various \nsolutions on table and choosing the best one out of the Big Data stack as per \nproblem statement. \n• Implemented various big data techniques i.e. creation scripts for Imported/Exported \nfrom different data sources target system using sqoop.  \n• Data ingestion into HDFS using Https Curl in BT HaaS env. Building Hive tables \non the data present in HDFS \n• ETL operation using Spark, PIG and HIVE for data transformations. \n• Reporting requirements are done thru. Impala tool for optimized performances. \n• Workflow orchestration and scheduling done thru. Hue / Oozie combination. \n• Performed a techno-commercial role.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":6,"lines":{"from":16,"to":30}}}}],["8ae6023b-5db2-46a5-a958-bdb0ab303dfc",{"pageContent":"• Reporting requirements are done thru. Impala tool for optimized performances. \n• Workflow orchestration and scheduling done thru. Hue / Oozie combination. \n• Performed a techno-commercial role. \n• Grooming team members with exhaustive functional / technical knowledge to bring \nthem up to speed. \n \n \nTitle iConectiv – Sr. Big Data Developer \nClient iConectiv – RPV Project \nPlatform Big Data, Spark, Scala, Python, Linux, MySQL, Rest WS, Shell, Agile Methodology, \nDescription RPV is iconectiv Right Party Verification (Right Party Verification), a Messaging solution \nsubscription-based service that delivers a way to verify and validate contact information as \npart of the requirement to be following the Telephone Consumer Protection Act (TCPA) \nlaws \n \nAssignments \n• Overall response for smooth delivery of RPV 2.1 Design, Development, \nDeployment till production support phase. \n• Worked with IC team closely to understand requirement and converting them into \nDesign documents.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":6,"lines":{"from":28,"to":47}}}}],["a599a819-6620-48ca-854c-49b9db9005fa",{"pageContent":"Deployment till production support phase. \n• Worked with IC team closely to understand requirement and converting them into \nDesign documents. \n• Understand iConectiv technical environment and setup DEV / TEST VM running \nRPV 2.0 application successfully. \n• Setup technical environment by setting up database & configuring different \nenvironment related properties / xml files. \n• Worked out estimation / project plan with client after converting business \nrequirement into technical requirement. \n• Led a team of developers & tester to ensure smoothly delivery. \n• Planning & executing ramp up plan for existing functionality plus addition of new \nfunctionality. \n• Troubleshooting / diagnosis issues across different layers @application/server level \n• Work stack management & team management to help facilitate team. \n• Regular calls with client for daily status update.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":6,"lines":{"from":45,"to":59}}}}],["9bc42dfc-09a9-4ccc-a50d-1a95ed7ce21d",{"pageContent":"• Ramped up team & involved in grooming team members with different functional / \ntechnical knowledge to bring them up to speed. \n \n \nTitle Airport Management System – Abu Dhabi & India – Big Data Developer \nClient SITA \nPlatform Java, Big Data, Spark, Hadoop, MapReduce, Pig, Hive, Sqoop, Scala, SAS, WMB, IBM \nMQ, Glassfish, Sql Lite, Sql Server, Spring Integration, XML, XSLT, XSD, MAVEN, Unix, \nJIRA, Jenkins, GIT, Gerrit, Scrum Methodology, AWS, Mongo DB \nDescription This project involves providing airport management solution systems for multiple airports \nfor meeting their end to end business needs and transformation of their existing stack from \nJava Spring Integration based solution to Big Data based stack using Hadoop ecosystem. \nFunctionally, AMS system helps in managing airport resources in best optimal manner and \nprimary airport stakeholders can better view / manage airport resources thru. dashboard \nconsole and allocate them efficiently which helps increase airport ROI.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":7,"lines":{"from":3,"to":17}}}}],["11be2767-f2b0-4b8a-8140-993a1a2d521d",{"pageContent":"primary airport stakeholders can better view / manage airport resources thru. dashboard \nconsole and allocate them efficiently which helps increase airport ROI. \n \nAssignments \n• Active participation in creating Big Data transformation design documents, setting \nup Hadoop clusters, leading development, deployment & testing team of AMS and \nJava adaptors at different AAI sites. \n• Implement, Maintain and troubleshoot Big Data Infrastructure. \n• Experience in Build and Maintain Infrastructure to support Data collection, ETL \nand Analysis. \n• Conversion of existing MR / Hive jobs to Spark platform using Spark QL & Spark \nStreaming. \n• Writing parallel processing jobs using Spark Scala, migrated to Spark Java later on. \n• Used Spark Scala / Java as primary parallel Processing engine for all different types \nof Batch / Real time processing in optimized manner. \n• Used Spark transformation / actions for Data enrichment, applying complex","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":7,"lines":{"from":16,"to":31}}}}],["d9ead5fb-7c05-4cc4-86bc-4f29390d9686",{"pageContent":"of Batch / Real time processing in optimized manner. \n• Used Spark transformation / actions for Data enrichment, applying complex \nbusiness logic to store normalized data in HDFS / No Sql DB. \n• Interfacing with different data sources like (MongoDB, HDFS, Files) to Extract, \nTransform and re-load Data \n• Design and Development of scalable and distributed applications using Hadoop \nTechnology Stack such as Apache Spark, Apache Hive and HDFS. \n• Implement and configure SITA's AMS software using customer requirements and \nbusiness process \n• Troubleshooting / diagnosis java to big data transformation issues across integration \nlayers @application/server level \n• Prepared a transformation design document for Java Adaptor framework conversion \nto Big Data data stack where each interface works as a pass thru. Interface between \nintegration across different vendor systems. \n• Utilized different Hadoop technologies MapReduce, Hive, Spark, Sqoop to","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":7,"lines":{"from":30,"to":44}}}}],["6ec71db0-b4f0-42a5-9223-7177dd72b119",{"pageContent":"integration across different vendor systems. \n• Utilized different Hadoop technologies MapReduce, Hive, Spark, Sqoop to \nfacilitate the transformation of business logic of AMS to Big Data ecosystem \n• Prepared the ICD, interface control documents, which details out the integration \nlayer across big data / Hadoop specifications. \n• Responsible for overall sprint delivery every fortnightly and different key \nstakeholder’s management. \n• Performed a techno-commercial role with respect to standard product offerings with \nBig Data / Hadoop \n• Identify the services in the middle tier integration platform developed using \nMessage Broker V8 \n• Conduct user acceptance testing and secure customer sign-off \n• Grooming team members with technology stack of Big Data, Hadoop, SOA, Java, \nWMB concepts \n \n \n \nTitle Data Acquisition System – UK, Dubai","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":7,"lines":{"from":43,"to":60}}}}],["20d4707e-d2d7-4487-94a9-9727b3652dfd",{"pageContent":"Client SITA \nPlatform Big Data, Hadoop, HDFS, Map Reduce, Spark, Hive, Java, Spring IOC, Hibernate, Python, \nSpring JMS Template, Spring JMX, MAVEN, IBM MQ, Active MQ, LDAP, JNDI, ANT, \nOracle 11g, Unix, JIRA, Scrum, Methodology \nDescription Data Acquisition System is core part of Border Security System that is implemented for \ndifferent countries (Canada, Mexico, Italy, and UAE etc.). It receives different type of \nmessages & structured / unstructured data generated at different point of time at airport like \nboarding, check-in and flight take off etc., from different sources, processes them according \nto the country, normalizes them and then send them to different downstream applications to \nensure proper vetting of passengers before flying. \n \nAssignments \n• Understanding and analyzing the transformational requirements to Big Data from \nlegacy data capture system. \n• Successfully Implemented the Big Data / Hadoop ecosystem while sourcing data","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":8,"lines":{"from":3,"to":17}}}}],["350cee4d-5893-40e3-8aa6-3c7fd9736bf1",{"pageContent":"• Understanding and analyzing the transformational requirements to Big Data from \nlegacy data capture system. \n• Successfully Implemented the Big Data / Hadoop ecosystem while sourcing data \nfeeds from different upstream application spitting both structured / unstructured data. \n• System Designing & Estimation as per Agile model to develop and enhance different \nmodules of the project. \n• Actively involved in overseeing Hadoop development team and responsible for sprint \ndelivery. \n• Status tracking across various threads. \n• Interfacing with various stakeholders (business, infrastructure, Quality Assurance )  \n• Project Planning and reporting to various stakeholders \n• To communicate with client and counterparts at client location whenever required. \n• To  Write  technical  specifications  including  Release  Documents  and  basic  flow \ndiagrams \n• To be part of Requirement Gathering Team \n• To mentor junior/new resources to the Big Data technologies, that are being used in","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":8,"lines":{"from":15,"to":30}}}}],["1306231a-b7ca-45f7-8440-3b2d79a2a365",{"pageContent":"diagrams \n• To be part of Requirement Gathering Team \n• To mentor junior/new resources to the Big Data technologies, that are being used in \nproject and make them familiar with the architecture of the project. \n• To deploy the application on AWS cloud and replica on local VM cloud. \n \nTitle BT Retail – Consumer.com \nClient BT \nPlatform WebLogic, Splunk, Run Deck, Solaris, Linux, Java, Unix, JIRA, Scrum Methodology, \nDescription Consumer.com is one of the Retail critical component, whereas all legacy OSCH based \napplications of BT are being shifted to new advance java technology. There were total of 22 \napps under consumer.com that are getting ramped up in a systematic manner by increase \ntraffic on the existing functionality plus addition of new features time to time. \n \nAssignments \n• Overall response for smooth delivery of consumer.com apps from deployment / \nsupport / incident management perspective.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":8,"lines":{"from":28,"to":44}}}}],["ac0537e9-3b27-44da-9ae0-023abe4ef4d9",{"pageContent":"Assignments \n• Overall response for smooth delivery of consumer.com apps from deployment / \nsupport / incident management perspective. \n• Planning & executing ramp up plan for existing functionality plus addition of new \nfunctionality. \n• Stakeholder management – Scrum masters, Product owners & BT Client. \n• Troubleshooting / diagnosis issues across different layers @application/server level \n• Work stack management & team management to help facilitate team to deliver stuff \non time without compromising on quality. \n• Regular calls with different stakeholder for status update and escalation \nmanagement. \n• Execution & tracking of AIS testing before every major release & secure major sign \noff. \n• Laid down and review different monitoring process across different consumer apps. \n• Review of deployment & resource plan before each deployment and sanity checks \npost major deployment. \n• Responsible for smooth execution of sprint delivery every fortnightly and different","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":8,"lines":{"from":42,"to":58}}}}],["5c16d9ff-5fa4-4a02-b90b-baccf951b0ef",{"pageContent":"• Review of deployment & resource plan before each deployment and sanity checks \npost major deployment. \n• Responsible for smooth execution of sprint delivery every fortnightly and different \nkey stakeholder’s management. \n• Performed a techno-commercial role.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":8,"lines":{"from":56,"to":60}}}}],["c5d9c88b-bfd4-4b5c-8cc1-d0e3909f8f17",{"pageContent":"• Grooming team members with different functional / technical knowledge to bring \nthem upto speed. \n \n \n \nTitle E-visa - Oman \nClient SITA \nPlatform Java, Glassfish, Oracle, SOA, OSB, XML, XSD, MAVEN, Unix, Jenkins, LIferay, LAMP \nDescription This project helps Oman ROP to facilitate visa processing services thru. online mechanism. \nOverall objective of this project to help travellers securing visa electronically in much faster \nmode, without having to personally visit embassy. Apart from providing the visa, this project \nalso does a necessary security check and vetting process before providing a visa. This project \nspans thru. integration stack layer built up on Glassfish, SOA, OSB & liferay etc. \ntechnology. \n \nAssignments \n• Part of Design, development & deployment / integration team overseeing various \ndevelopment related activities \n• Troubleshooting various development, deployment & integration issues. \n• Set up cluster environment for Glassfish, SOA, OSB & Liferay.","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":9,"lines":{"from":3,"to":22}}}}],["61f8802b-8ea9-4a5b-9554-82c9e6b14eb0",{"pageContent":"development related activities \n• Troubleshooting various development, deployment & integration issues. \n• Set up cluster environment for Glassfish, SOA, OSB & Liferay. \n• For every sprint release, to ensure successful deployment on Glassfish, SOA, OSB \n& liferay on multiple environments. \n• Liaising with different support teams like Infra / DBA for any technical issues. \n• Troubleshooting / diagnosis issues across integration layers @application/server \nlevel \n• Providing support to different testing teams with respect to their environmental & \ntechnical issues. \n• Responsible for timely handover of various environments after successfully \ndeployment to different testing teams. \n• Grooming the team members with respect to development, deployment & \nintegration issues. \n \n \n \nTitle AQS Middle Office, US \nClient Quadriserv Inc. NY \nOrganization IVP \nPlatform Java, J2EE, Spring, Weblogic Server, OSB, JMS, POJOs, Top link, Quartz scheduler, Shell scripts, \nWeb services, Unix env","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":9,"lines":{"from":20,"to":41}}}}],["4bf7f03e-12e9-4784-a863-1800d9eda9c8",{"pageContent":"Client Quadriserv Inc. NY \nOrganization IVP \nPlatform Java, J2EE, Spring, Weblogic Server, OSB, JMS, POJOs, Top link, Quartz scheduler, Shell scripts, \nWeb services, Unix env \nProject \nDescription \nAQS Middle Office (AQS: MO) is a fully scaled j2ee platform to automate the daily actions of post-\ntransaction processing on the trades done through the AQS. AQS: MO is responsible for processing all \nstandard events affecting a stock loan and updates each open position on a daily basis. These events \ninclude mark-to-market valuation, corporate actions, and all partial or full recalls, returns, re-rates. \nMatched borrows and loans are sent from AQS directly to AQS:MO where the matched pair is \nvalidated before it is forwarded to OCC. OCC, as the central counterparty, passes the matched pair to \nDTC for settlement. DTC then confirms that the shares and collateral are in place in the appropriate","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":9,"lines":{"from":38,"to":50}}}}],["1c6c524f-4cf1-475e-965a-b876170355b5",{"pageContent":"DTC for settlement. DTC then confirms that the shares and collateral are in place in the appropriate \ncounterparty’s account before it settles the transaction. Upon confirmation of settlement by DTC, \nacknowledgement is sent to OCC and passed on to AQS. Members may view the status of their \ntransactions in AQS on a real-time basis. \n \nResponsibilities \n• Responsible for weblogic / OSB based deployment. \n• Involved in troubleshooting / diagnosis at application / server level. \n• Weblogic server configuration. \n• Weblogic deployment thru. WLST / Web console. \n• Monitoring OS/DB level alerts and provides quick resolution. \n• Produce Deployment, Run Books and Implementation Plans \n• Develops scripts (ANT, or any build script) and automation tools used to build, integrate, and \ndeploy software releases to various platforms","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":9,"lines":{"from":50,"to":63}}}}],["d73d6764-5e1c-4bd4-9ad6-441a7d237a5d",{"pageContent":"• Maintains a release repository and manages key information such as build and release \nprocedures, dependencies, and notification lists \n• Managing all aspects of AQS: MO - high visibility project for Quadriserv Inc. peak team size \n-21 \n• Modules technical analysis, coding and architecture design for AQS Middle Office project \nand all associated project threads and lead project management.    \n• End to end management technical as well as functional of various threads (Middle Office, \nAQS API (java/ Dot net), MIS , Market Data,loanet Adaptor)  and design activities on all \nprojects coordinating with multiple teams  \n• Conceptualized and managed multiple design/development iterations of the AQS Middle \nOffice and MIS  \n• Implemented agile development methodology across various threads current as well as past in \nthe Middle office project. \n• Status tracking across various threads. \n• Interfacing with various stakeholders (business, infrastructure, Quality Assurance )","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":10,"lines":{"from":3,"to":17}}}}],["357f3ede-457e-4b9e-a5c7-ce413bd6c5ec",{"pageContent":"the Middle office project. \n• Status tracking across various threads. \n• Interfacing with various stakeholders (business, infrastructure, Quality Assurance )  \n \n \n \n \n \n \nTitle Eclipse (Portfolio Management System), UK \nClient Deutsche Bank – NY / London \nOrganization IVP \nPlatform J2EE, EJB, Weblogic, JSP, Servlet, Perl, Unix, Sybase \nDescription This system provides the provision of a suite of services to Hedge Funds comprising \nclearance and settlement, reporting, financing and securities lending. All these services are \nprovided by the Eclipse application and its sub-components, or through links to other \nDeutsche Bank systems.   One of the key roles of this application is valuation process, which \nis required to calculate client margin requirement for each fund (called portfolio in Eclipse). \nThis process values each position and calculates the margin requirement for each position or \nhedged position. The market value and margin requirement is then summed up by strategy","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":10,"lines":{"from":15,"to":34}}}}],["b286a7d7-eb6b-4b95-820e-3e7f648f8cfc",{"pageContent":"This process values each position and calculates the margin requirement for each position or \nhedged position. The market value and margin requirement is then summed up by strategy \nand then totaled for each fund. In this process, the accrued coupon interest is also accrued for \neach convertible bond position. This application has got the clients in US/UK/Asia and \nsupported by \nAssignments \n• Involved in sorting out daily issues which involves Valuation process, Trade file, \nExporting reports issues \n• Supporting development team to resolve functional queries.  \n• Demonstrating software solutions to the clients and prospects.  \n• Monitor the different number of batches feeding to and from eclipse and other \nsystems involves in the whole process","metadata":{"source":"/tmp/resume-processor-SoN0mW/Sandeep_Resume.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.3","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"macOS Version 13.5.1 (Build 22G90) Quartz PDFContext","CreationDate":"D:20230928182930Z00'00'","ModDate":"D:20230928182930Z00'00'"},"metadata":null,"totalPages":10},"loc":{"pageNumber":10,"lines":{"from":33,"to":44}}}}]],{"0":"f10cfa4a-fb08-45f4-bd79-64a968cfb9b8","1":"1492048b-c12a-4155-a69f-152b51a7cdb0","2":"34c7b52e-9361-4152-bb5c-6f6eaefc1d3f","3":"142ffd59-1863-4af1-ac7d-930b4546e8c7","4":"a456afe3-1833-4a91-834e-95e08f1d9885","5":"c0d8adb6-d748-4d71-bf7d-a6b98fa06e3e","6":"593869ee-0322-4fa1-b8c0-bfc3e76a1a59","7":"28f86679-a787-4fad-b7f4-d9c43374889d","8":"c4491f3a-54fe-44bd-918f-2ed25d5d6a45","9":"f748318a-e4f2-457d-8205-8055a83ac354","10":"ba83840f-906a-4a21-90fb-7e8d50dbad8d","11":"b49d8c1c-a11e-46d5-9ef4-b848e6406572","12":"5ec117a7-8547-46f7-81b5-cfd9be542939","13":"a6d6704f-2da0-4310-a7b8-2f40525590b8","14":"a30db374-af18-44cc-9eda-df8afb39ee4e","15":"2b51906f-4819-4062-8ad0-f595fbf91c02","16":"83c53a86-386a-4955-a9b6-a5dd715211ba","17":"5282733b-8092-493f-8ab8-a3f09ab46823","18":"d5f0cf35-932d-4e35-8538-305c3f2d2d11","19":"d10479dc-931d-4c2e-863a-d97e3be403f1","20":"3a008d76-21a6-4a6b-a47d-bf3b287374f1","21":"8ae6023b-5db2-46a5-a958-bdb0ab303dfc","22":"a599a819-6620-48ca-854c-49b9db9005fa","23":"9bc42dfc-09a9-4ccc-a50d-1a95ed7ce21d","24":"11be2767-f2b0-4b8a-8140-993a1a2d521d","25":"d9ead5fb-7c05-4cc4-86bc-4f29390d9686","26":"6ec71db0-b4f0-42a5-9223-7177dd72b119","27":"20d4707e-d2d7-4487-94a9-9727b3652dfd","28":"350cee4d-5893-40e3-8aa6-3c7fd9736bf1","29":"1306231a-b7ca-45f7-8440-3b2d79a2a365","30":"ac0537e9-3b27-44da-9ae0-023abe4ef4d9","31":"5c16d9ff-5fa4-4a02-b90b-baccf951b0ef","32":"c5d9c88b-bfd4-4b5c-8cc1-d0e3909f8f17","33":"61f8802b-8ea9-4a5b-9554-82c9e6b14eb0","34":"4bf7f03e-12e9-4784-a863-1800d9eda9c8","35":"1c6c524f-4cf1-475e-965a-b876170355b5","36":"d73d6764-5e1c-4bd4-9ad6-441a7d237a5d","37":"357f3ede-457e-4b9e-a5c7-ce413bd6c5ec","38":"b286a7d7-eb6b-4b95-820e-3e7f648f8cfc"}]